Project Goal

Train a model to predict REF star ratings (4\*, 3\*, 2\*, 1\*) from the full text of REF outputs in UoA 4 (Psychology).

Fine-Tuning Pipeline: Start to Finish


1. Data Preparation

I started with:

600 full-text REF output PDFs
DOIs matched to each PDF
Labeled outputs (REF 2021 data) with institutional-level star distribution

I:

Extracted text from each PDF 
Mapped each output to a label (based on its institutionâ€™s most common score in UoA 4) 
Saved as a JSON file: `ref_training_data.json`

Each entry looked like:

```json
{"text": "...full-text content...", "label": "4*"}
```



2. Dataset Loading & Label Encoding

```python
dataset = load_dataset("json", data_files="ref_training_data.json", split="train")
```

Used Hugging Face `datasets` to load the JSON

Then:

```python
label_map = {"4*": 0, "3*": 1, "2*": 2, "1*": 3}
```

Star ratings were converted to integer class IDs for the model (multi-class classification)


3. Tokenization (Text â†’ IDs)

```python
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

I used DistilBERT: a fast, lightweight transformer pretrained on English text.

```python
def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=512)
```

Each text was truncated or padded to 512 tokens (max input size for DistilBERT)


4. Model Architecture

```python
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=4
)
```

This loads a DistilBERT model with a new classification head:

Final layer: 4 output neurons â†’ each for a REF star class
New layers (`classifier.weight`, etc.) were randomly initialized â€” this is why you trained the model



5. Training Configuration

```python
training_args = TrainingArguments(
    output_dir="./ref_model",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    save_steps=10,
    save_total_limit=2,
    logging_steps=5,
    report_to="none"
)
```

 Batch size: 4 samples at a time
 Epochs: 3 full passes through the dataset
 Saves model every few steps (and at end)
 Logging loss during training


6. Training Execution

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized,
    tokenizer=tokenizer
)

trainer.train()
```

Hugging Face `Trainer` handled batching, shuffling, backpropagation, etc.
Optimizer used: AdamW
Loss function: CrossEntropyLoss (standard for multi-class classification)


7. Saving The Model

```python
trainer.save_model("ref_star_classifier")
tokenizer.save_pretrained("ref_star_classifier")
```

I saved:

 The trained weights (`pytorch_model.bin`)
 Model config
 Tokenizer vocab

This directory can now be reused for prediction, sharing, or fine-tuning further.

Summary

| Step         | Action                                                              |
| ------------ | ------------------------------------------------------------------- |
| ðŸ“„ Dataset   | 600 full-text REF outputs labeled using institutional UoA 4 ratings |
| ðŸ”¤ Tokenizer | DistilBERT tokenizer (`max_length=512`)                             |
| ðŸ§  Model     | `distilbert-base-uncased` + 4-class classifier head                 |
| ðŸŽ“ Training  | 3 epochs, batch size 4, loss decreasing steadily                    |
| ðŸ’¾ Output    | Model saved to `ref_star_classifier/`                               |
| ðŸ§ª Inference | Now used to predict new PDF scores                                  |

